# save_samples.py
# ç”»åƒãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã‚¹ã‚¯ãƒªãƒ—ãƒˆ

import hashlib
import os
import sys
import time
from pathlib import Path

import requests
from dotenv import load_dotenv

# è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿
try:
    from config import CONFIG
except ImportError:
    print("âŒ ã‚¨ãƒ©ãƒ¼: config.py ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
    print("config.template.py ã‚’ config.py ã«ã‚³ãƒ”ãƒ¼ã—ã¦ã‹ã‚‰å®Ÿè¡Œã—ã¦ãã ã•ã„:")
    print("  copy config.template.py config.py")
    sys.exit(1)

# .envãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ç’°å¢ƒå¤‰æ•°ã‚’èª­ã¿è¾¼ã¿
load_dotenv()

def sha256_bytes(b):
    return hashlib.sha256(b).hexdigest()


def ensure_dir(p):
    Path(p).mkdir(parents=True, exist_ok=True)

def cleanup_empty_folder(folder_path, was_newly_created):
    """ç©ºã®ãƒ•ã‚©ãƒ«ãƒ€ã‚’å‰Šé™¤ã™ã‚‹ï¼ˆæ–°è¦ä½œæˆã•ã‚ŒãŸå ´åˆã®ã¿ï¼‰"""
    if not was_newly_created:
        return False
    
    try:
        folder = Path(folder_path)
        if folder.exists() and folder.is_dir():
            # ãƒ•ã‚©ãƒ«ãƒ€å†…ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒã‚§ãƒƒã‚¯
            files = list(folder.glob("*"))
            if not files:  # ç©ºã®å ´åˆ
                folder.rmdir()
                print(f"ğŸ—‘ï¸  ç©ºã®ãƒ•ã‚©ãƒ«ãƒ€ã‚’å‰Šé™¤ã—ã¾ã—ãŸ: {folder}")
                return True
    except Exception as e:
        print(f"âš ï¸  ãƒ•ã‚©ãƒ«ãƒ€å‰Šé™¤ã«å¤±æ•—: {e}", file=sys.stderr)
    return False


def sanitize_folder_name(name):
    """ãƒ•ã‚©ãƒ«ãƒ€åã¨ã—ã¦ä½¿ç”¨ã§ããªã„æ–‡å­—ã‚’ç½®æ›ã™ã‚‹"""
    # Windowsã§ä½¿ç”¨ã§ããªã„æ–‡å­—ã‚’ç½®æ›
    invalid_chars = '<>:"/\\|?*'
    for char in invalid_chars:
        name = name.replace(char, '_')
    # é€£ç¶šã™ã‚‹ã‚¹ãƒšãƒ¼ã‚¹ã‚’ã‚¢ãƒ³ãƒ€ãƒ¼ã‚¹ã‚³ã‚¢ã«ç½®æ›
    name = '_'.join(name.split())
    # å…ˆé ­ãƒ»æœ«å°¾ã®ä¸è¦ãªæ–‡å­—ã‚’å‰Šé™¤
    name = name.strip('._')
    return name if name else 'unknown'


def download(url, outdir):
    r = requests.get(url, timeout=30)
    r.raise_for_status()
    ext = ".jpg"
    h = sha256_bytes(r.content)[:16]
    fp = Path(outdir) / f"{h}{ext}"
    if not fp.exists():
        with open(fp, "wb") as f:
            f.write(r.content)
    return str(fp)

def search_pixabay(api_key, query, per_page, page, min_w=None, min_h=None):
    params = {
        "key": api_key, 
        "q": query, 
        "image_type": "photo",
        "safesearch": "true", 
        "per_page": per_page, 
        "page": page,
    }
    if min_w:
        params["min_width"] = min_w
    if min_h:
        params["min_height"] = min_h
    
    url = "https://pixabay.com/api/"
    r = requests.get(url, params=params, timeout=30)
    r.raise_for_status()
    data = r.json()
    
    # ãƒ¬ãƒ¼ãƒˆåˆ¶é™æƒ…å ±ã‚’å–å¾—
    rate_limit_info = {
        'limit': r.headers.get('X-RateLimit-Limit'),
        'remaining': r.headers.get('X-RateLimit-Remaining'),
        'reset': r.headers.get('X-RateLimit-Reset')
    }
    
    # æ¤œç´¢çµæœæƒ…å ±ã‚’å–å¾—
    search_info = {
        'total': data.get('total', 0),
        'totalHits': data.get('totalHits', 0),
        'current_page': page,
        'per_page': per_page
    }
    
    # ç”»åƒURL: largeImageURL or webformatURL
    urls = [
        hit.get("largeImageURL") or hit.get("webformatURL") 
        for hit in data.get("hits", []) 
        if hit.get("largeImageURL") or hit.get("webformatURL")
    ]
    return urls, rate_limit_info, search_info

def search_pexels(api_key, query, per_page, page):
    headers = {"Authorization": api_key}
    params = {"query": query, "per_page": per_page, "page": page}
    url = "https://api.pexels.com/v1/search"
    r = requests.get(url, headers=headers, params=params, timeout=30)
    r.raise_for_status()
    data = r.json()
    
    # æ¤œç´¢çµæœæƒ…å ±ã‚’å–å¾—
    search_info = {
        'total': data.get('total_results', 0),
        'totalHits': data.get('total_results', 0),  # Pexelsã¯åŒã˜å€¤
        'current_page': page,
        'per_page': per_page
    }
    
    # ç”»åƒURLå€™è£œ: original (æœ€å¤§), large, large2x, etc.
    urls = []
    for p in data.get("photos", []):
        src = p.get("src", {})
        url_candidate = (
            src.get("original") or 
            src.get("large2x") or 
            src.get("large") or 
            src.get("medium")
        )
        urls.append(url_candidate)
    urls = [u for u in urls if u]
    return urls, search_info

def display_rate_limit_info(rate_info):
    """ãƒ¬ãƒ¼ãƒˆåˆ¶é™æƒ…å ±ã‚’è¡¨ç¤ºã™ã‚‹"""
    if any(rate_info.values()):
        print("ğŸ“Š APIåˆ¶é™æƒ…å ±:")
        if rate_info['limit']:
            print(f"   åˆ¶é™æ•°: {rate_info['limit']}/60ç§’")
        if rate_info['remaining']:
            print(f"   æ®‹ã‚Š: {rate_info['remaining']}ãƒªã‚¯ã‚¨ã‚¹ãƒˆ")
        if rate_info['reset']:
            print(f"   ãƒªã‚»ãƒƒãƒˆã¾ã§: {rate_info['reset']}ç§’")
        print()


def display_search_info(search_info, provider, folder_path=None, 
                       was_newly_created=False):
    """æ¤œç´¢çµæœæƒ…å ±ã‚’è¡¨ç¤ºã™ã‚‹"""
    total = search_info.get('total', 0)
    totalHits = search_info.get('totalHits', 0)
    
    print("ğŸ” æ¤œç´¢çµæœ:")
    print(f"   ç·ä»¶æ•°: {total:,} ä»¶")
    print(f"   åˆ©ç”¨å¯èƒ½: {totalHits:,} ä»¶")
    
    if total == 0:
        print(f"âŒ ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ '{CONFIG['query']}' ã®æ¤œç´¢çµæœãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
        print("   ğŸ’¡ æ¤œç´¢ã®ãƒ’ãƒ³ãƒˆ:")
        print("      - è‹±èªã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’è©¦ã—ã¦ãã ã•ã„")
        print("      - ã‚ˆã‚Šä¸€èˆ¬çš„ãªå˜èªã‚’ä½¿ç”¨ã—ã¦ãã ã•ã„") 
        print("      - ã‚¹ãƒšãƒ«ã‚’ç¢ºèªã—ã¦ãã ã•ã„")
        if (provider == "pixabay" and 
            (CONFIG.get('min_width') or CONFIG.get('min_height'))):
            print("      - æœ€å°ã‚µã‚¤ã‚ºåˆ¶é™ã‚’ç·©å’Œã—ã¦ãã ã•ã„")
        
        # æ–°è¦ä½œæˆã•ã‚ŒãŸãƒ•ã‚©ãƒ«ãƒ€ãŒã‚ã‚‹å ´åˆã¯å‰Šé™¤
        if folder_path and was_newly_created:
            cleanup_empty_folder(folder_path, was_newly_created)
        
        return False
    elif totalHits == 0:
        print("âŒ æ¤œç´¢çµæœã¯è¦‹ã¤ã‹ã‚Šã¾ã—ãŸãŒã€åˆ©ç”¨å¯èƒ½ãªç”»åƒãŒã‚ã‚Šã¾ã›ã‚“ã€‚")
        
        # æ–°è¦ä½œæˆã•ã‚ŒãŸãƒ•ã‚©ãƒ«ãƒ€ãŒã‚ã‚‹å ´åˆã¯å‰Šé™¤
        if folder_path and was_newly_created:
            cleanup_empty_folder(folder_path, was_newly_created)
            
        return False
    else:
        print("âœ… æ¤œç´¢æˆåŠŸï¼")
        return True

def main():
    # è¨­å®šã‚’èª­ã¿è¾¼ã¿
    provider = CONFIG["provider"]
    query = CONFIG["query"]
    n = CONFIG["n"]
    min_width = CONFIG["min_width"]
    min_height = CONFIG["min_height"]
    delay = CONFIG["delay"]
    
    # .envãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ä¿å­˜å…ˆãƒ•ã‚©ãƒ«ãƒ€ã‚’å–å¾—
    base_folder = os.getenv("SAVE_FOLDER", "./samples")
    
    # ã‚¯ã‚¨ãƒªç”¨ã®ã‚µãƒ–ãƒ•ã‚©ãƒ«ãƒ€ã‚’ä½œæˆ
    query_folder = sanitize_folder_name(query)
    out = Path(base_folder) / query_folder
    
    # .envãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰APIã‚­ãƒ¼ã‚’å–å¾—
    if provider == "pixabay":
        api_key = os.getenv("PIXABAY_API_KEY")
    elif provider == "pexels":
        api_key = os.getenv("PEXELS_API_KEY")
    else:
        print(f"ã‚¨ãƒ©ãƒ¼: å¯¾å¿œã—ã¦ã„ãªã„ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã§ã™: {provider}")
        return
    
    # APIã‚­ãƒ¼ã®ãƒã‚§ãƒƒã‚¯
    if not api_key:
        print(f"ã‚¨ãƒ©ãƒ¼: {provider.upper()}_API_KEY ãŒ .env ãƒ•ã‚¡ã‚¤ãƒ«ã«"
              "è¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“")
        print(f".env ãƒ•ã‚¡ã‚¤ãƒ«ã« {provider.upper()}_API_KEY="
              "your_actual_api_key ã‚’è¿½åŠ ã—ã¦ãã ã•ã„")
        return
    
    # æ—¢å­˜ã®ãƒ•ã‚¡ã‚¤ãƒ«æ•°ã‚’ãƒã‚§ãƒƒã‚¯
    folder_was_newly_created = not out.exists()
    ensure_dir(out)
    existing_files = list(Path(out).glob("*.jpg"))
    existing_count = len(existing_files)
    
    print(f"ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼: {provider}")
    print(f"æ¤œç´¢ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰: {query}")
    print(f"ä¿å­˜å…ˆ: {out}")
    print(f"æ—¢å­˜ãƒ•ã‚¡ã‚¤ãƒ«æ•°: {existing_count}")
    print(f"ç›®æ¨™ãƒ•ã‚¡ã‚¤ãƒ«æ•°: {n}")
    
    if existing_count >= n:
        print(f"âœ… æ—¢ã« {existing_count} å€‹ã®ãƒ•ã‚¡ã‚¤ãƒ«ãŒã‚ã‚Šã¾ã™ã€‚"
              f"ç›®æ¨™æ•° {n} ã«é”æ¸ˆã¿ã§ã™ã€‚")
        return
    
    additional_needed = n - existing_count
    print(f"è¿½åŠ ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰äºˆå®š: {additional_needed} å€‹")
    
    if min_width or min_height:
        print(f"æœ€å°ã‚µã‚¤ã‚º: {min_width}x{min_height}")
    print("ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰é–‹å§‹...")
    print()
    
    got = 0
    page = 1
    per_page = min(additional_needed, 80)
    rate_info_displayed = False
    search_info_displayed = False
    consecutive_empty_pages = 0
    max_empty_pages = 3  # é€£ç¶šã—ã¦ç©ºã®ãƒšãƒ¼ã‚¸ãŒç¶šã„ãŸå ´åˆã®ä¸Šé™
    while got < additional_needed:
        try:
            if provider == "pixabay":
                urls, rate_info, search_info = search_pixabay(
                    api_key, query, per_page, page, min_width, min_height
                )
                # æœ€åˆã®ãƒªã‚¯ã‚¨ã‚¹ãƒˆå¾Œã«ãƒ¬ãƒ¼ãƒˆåˆ¶é™æƒ…å ±ã‚’è¡¨ç¤º
                if not rate_info_displayed:
                    display_rate_limit_info(rate_info)
                    rate_info_displayed = True
                # æ¤œç´¢çµæœæƒ…å ±ã®è¡¨ç¤ºï¼ˆæœ€åˆã®ãƒšãƒ¼ã‚¸ã®ã¿ï¼‰
                if not search_info_displayed:
                    if not display_search_info(
                        search_info, provider, out, folder_was_newly_created
                    ):
                        return  # æ¤œç´¢çµæœãŒãªã„å ´åˆã¯çµ‚äº†
                    search_info_displayed = True
                    print()
            else:
                urls, search_info = search_pexels(api_key, query, per_page, page)
                # æ¤œç´¢çµæœæƒ…å ±ã®è¡¨ç¤ºï¼ˆæœ€åˆã®ãƒšãƒ¼ã‚¸ã®ã¿ï¼‰
                if not search_info_displayed:
                    if not display_search_info(
                        search_info, provider, out, folder_was_newly_created
                    ):
                        return  # æ¤œç´¢çµæœãŒãªã„å ´åˆã¯çµ‚äº†
                    search_info_displayed = True
                    print()

            if not urls:
                consecutive_empty_pages += 1
                if consecutive_empty_pages >= max_empty_pages:
                    print(f"âš ï¸  é€£ç¶šã—ã¦ {max_empty_pages} ãƒšãƒ¼ã‚¸ã«çµæœãŒã‚ã‚Šã¾ã›ã‚“ã€‚"
                          "æ¤œç´¢ã‚’çµ‚äº†ã—ã¾ã™ã€‚")
                    break
                print(f"ğŸ“„ ãƒšãƒ¼ã‚¸ {page}: çµæœãªã— "
                      f"({consecutive_empty_pages}/{max_empty_pages})")
                page += 1
                continue
            else:
                consecutive_empty_pages = 0  # ãƒªã‚»ãƒƒãƒˆ

        except requests.exceptions.RequestException as e:
            print(f"âŒ API ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚¨ãƒ©ãƒ¼: {e}", file=sys.stderr)
            print("   ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯æ¥ç¶šã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚", file=sys.stderr)
            break
        except Exception as e:
            print(f"âŒ äºˆæœŸã—ãªã„ã‚¨ãƒ©ãƒ¼: {e}", file=sys.stderr)
            break

        for u in urls:
            try:
                fp = download(u, out)
                got += 1
                total_files = existing_count + got
                print(f"[{total_files}] {fp}")
                if got >= additional_needed:
                    break
                time.sleep(delay)
            except Exception as e:
                print(f"skip {u}: {e}", file=sys.stderr)
        page += 1

    final_count = existing_count + got
    if got > 0:
        print(f"âœ… å®Œäº†: {got} å€‹ã®æ–°ã—ã„ãƒ•ã‚¡ã‚¤ãƒ«ã‚’è¿½åŠ ã—ã¾ã—ãŸ")
        print(f"ğŸ“ ç·ãƒ•ã‚¡ã‚¤ãƒ«æ•°: {final_count} å€‹ ({out})")
    else:
        print("âŒ æ–°ã—ã„ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã§ãã¾ã›ã‚“ã§ã—ãŸ")
        if final_count > 0:
            print(f"ğŸ“ æ—¢å­˜ãƒ•ã‚¡ã‚¤ãƒ«æ•°: {final_count} å€‹ ({out})")
        else:
            # ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã§ããšã€æ—¢å­˜ãƒ•ã‚¡ã‚¤ãƒ«ã‚‚ãªã„å ´åˆã€æ–°è¦ä½œæˆã•ã‚ŒãŸãƒ•ã‚©ãƒ«ãƒ€ã‚’å‰Šé™¤
            cleanup_empty_folder(out, folder_was_newly_created)

if __name__ == "__main__":
    main()